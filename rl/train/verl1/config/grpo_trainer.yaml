# GRPO训练器配置文件 - 基于verl标准格式的完整配置
# (已根据官方 config_verl 模板修正)
defaults:
  - _self_

# 自定义迭代式强化学习配置
# 这个模块被你的 iterative_grpo_trainer.py 脚本读取
iterative_rl:
  max_iterations: 2
  convergence_threshold: 0.01
  rollout_data_dir: "./a_model_grpo_standard/rollouts"
  iteration_log_dir: "./a_model_grpo_standard/iterations"
  system_prompt_count: 2
  code_generation_count: 2
  a_model:
    #model_path: "/data/zhuldz/lunwen/models/Qwen3-0.6B/v1"
    model_path: "/data/zhuldz/lunwen/models/Qwen3-4B"
    device_id: 0
    example_file: "/data/zhuldz/lunwen/rl/train/verl1/sysprompt_icl.txt"
    lora_r: 8
    lora_alpha: 16
    lora_dropout: 0.1

# 算法配置 - 使用GRPO
algorithm:
  # 目标类在 algorithm.py 中定义
  _target_: verl.trainer.config.AlgoConfig
  adv_estimator: grpo
  gamma: 1.0
  lam: 1.0
  norm_adv_by_std_in_grpo: true
  use_kl_in_reward: false
  rollout_is_threshold: null
  kl_penalty: kl
  kl_ctrl:
    _target_: verl.trainer.config.KLControlConfig
    type: fixed
    kl_coef: 0.001
    horizon: 10000
    target_kl: 0.1

# 训练器配置
# 参考: verl1/config_verl/ppo_trainer.yaml
trainer:
  total_epochs: 2
  project_name: verl_grpo_iterative
  experiment_name: qwen3_4b_code_generation
  logger: ['console', 'tensorboard']
  nnodes: 1
  n_gpus_per_node: 4
  save_freq: 5
  test_freq: -1
  critic_warmup: 0
  device: cuda
  default_local_dir: "./a_model_grpo_standard/checkpoints"
  val_only: false
  val_before_train: false
  seed: 42
  total_training_steps: null 
  resume_mode: auto
  default_hdfs_dir: null
  balance_batch: True
  log_val_generations: 0
  esi_redundant_time: 0
  del_local_ckpt_after_load: false
  rollout_data_dir: "./a_model_grpo_standard/rollouts"
  # 移除了 'use_amp', 'grad_clip' (应在actor/critic中), 'grad_accumulation_steps' 等非标准PPO trainer字段

# 数据配置
# 参考: verl1/config_verl/data/legacy_data.yaml
data:
  # 移除了 _target_ 和 SFT 特有的字段
  train_files: "/data/zhuldz/lunwen/data/OpenCodeInstruct/dataparquet/small_test1.parquet"
  val_files: /data/zhuldz/lunwen/data/CodeEval-Pro/dataset_1/humaneval_pro.parquet
  train_batch_size: 2
  micro_batch_size: 2
  val_batch_size: null
  max_prompt_length: 8192
  max_response_length: 2048
  shuffle: false
  reward_fn_key: "output"
  prompt_key: "input"
  dataloader_num_workers: 4 
  return_raw_chat: True
  filter_overlong_prompts: false

  # 修正了 sampler 的结构，如果不需要，保持 null 即可
  sampler:
    class_path: null
    class_name: null

# Actor模型配置
actor_rollout_ref:
  model:
    # 目标类在 hf_model.yaml 中定义
    _target_: verl.workers.config.HFModelConfig
    path: "/data/zhuldz/lunwen/models/Qwen3-0.6B/v1"
    use_remove_padding: true
    trust_remote_code: true
    enable_gradient_checkpointing: true
    custom_chat_template: null
    # --- 关键修正 ---
    # 'dtype' 和 'torch_dtype' 不属于 HFModelConfig，已移除

  actor:
    _target_: verl.workers.config.FSDPActorConfig
    strategy: fsdp
    use_remove_padding: true
    ppo_mini_batch_size: 2
    ppo_micro_batch_size: null
    ppo_micro_batch_size_per_gpu: 1
    use_dynamic_bsz: false
    use_kl_loss: false
    kl_loss_coef: 0.001
    kl_loss_type: "low_var_kl"
    clip_ratio: 0.2
    loss_agg_mode: "seq-mean-token-mean"
    #loss_agg_mode: "seq-mean-token-sum"
    entropy_coeff: 0.01
    ppo_epochs: 2
    shuffle: false
    use_torch_compile: true
    optim:
      # 'optim' 在 actor 内部定义是正确的
      _target_: verl.workers.config.FSDPOptimizerConfig
      lr: 1e-6
      betas: [0.9, 0.999]
      weight_decay: 0.01
    fsdp_config:
      _target_: verl.workers.config.FSDPEngineConfig
      wrap_policy:
        min_num_params: 0
      param_offload: true
      optimizer_offload: false
      offload_policy: false
      reshard_after_forward: true
      fsdp_size: -1
      forward_prefetch: false
      model_dtype: bfloat16
      use_orig_params: false
      ulysses_sequence_parallel_size: 1
      entropy_from_logits_with_chunking: false
      use_torch_compile: true
      entropy_checkpointing: false
      forward_only: false
      strategy: fsdp
    checkpoint:
      _target_: verl.trainer.config.CheckpointConfig
      save_contents:
      - model
      - optimizer
      - extra
      load_contents: ${.save_contents}
      async_save: false
    

  ref:
    # 'ref' 的配置看起来是正确的
    strategy: ${actor_rollout_ref.actor.strategy}
    use_torch_compile: ${oc.select:actor_rollout_ref.actor.use_torch_compile,true}
    log_prob_micro_batch_size: null
    log_prob_micro_batch_size_per_gpu: 1
    log_prob_use_dynamic_bsz: ${oc.select:actor_rollout_ref.actor.use_dynamic_bsz,false}
    log_prob_max_token_len_per_gpu: ${oc.select:actor_rollout_ref.actor.ppo_max_token_len_per_gpu,16384}
    
    profiler:
      _target_: verl.utils.profiler.ProfilerConfig
      tool: ${oc.select:global_profiler.tool,null}
      enable: false
      all_ranks: false
      ranks: []
      save_path: ${oc.select:global_profiler.save_path,null}
      tool_config:
        nsys:
          _target_: verl.utils.profiler.config.NsightToolConfig
          discrete: ${oc.select:global_profiler.global_tool_config.nsys.discrete}
        npu:
          _target_: verl.utils.profiler.config.NPUToolConfig
          contents: []
          level: level1
          analysis: true
          discrete: false
        torch:
          _target_: verl.utils.profiler.config.TorchProfilerToolConfig
          step_start: 0
          step_end: null
        torch_memory:
          _target_: verl.utils.profiler.config.TorchMemoryToolConfig
          trace_alloc_max_entries: ${oc.select:global_profiler.global_tool_config.torch_memory.trace_alloc_max_entries,100000}
          stack_depth: ${oc.select:global_profiler.global_tool_config.torch_memory.stack_depth,32}
    fsdp_config:
      _target_: verl.workers.config.FSDPEngineConfig
      wrap_policy:
        min_num_params: 0
      param_offload: true
      optimizer_offload: false
      offload_policy: false
      reshard_after_forward: true
      fsdp_size: -1
      forward_prefetch: false
      model_dtype: bfloat16
      use_orig_params: false
      ulysses_sequence_parallel_size: 1
      entropy_from_logits_with_chunking: false
      use_torch_compile: true
      entropy_checkpointing: false
      forward_only: false
      strategy: fsdp
    model: null
    ulysses_sequence_parallel_size: ${oc.select:actor_rollout_ref.actor.ulysses_sequence_parallel_size,1}
    entropy_from_logits_with_chunking: false
    entropy_checkpointing: false

  rollout:
    # 目标类在 rollout.yaml 中定义
    _target_: verl.workers.config.RolloutConfig
    name: vllm
    max_model_len: 8192
    n: 2
    log_prob_micro_batch_size: null
    log_prob_micro_batch_size_per_gpu: 1
    temperature: 1.0
    top_p: 1.0
    top_k: 50
    prompt_length: 512
    response_length: 2048
    dtype: bfloat16  # 'dtype' 属于这里，用于配置 vLLM/SGLang
    gpu_memory_utilization: 0.25
    ignore_eos: false
    enforce_eager: false
    free_cache_engine: true
    layered_summon: False
    # --- 关键修正 (逻辑错误) ---
    # 'n_gpus_per_node' 为 1，因此 'tensor_model_parallel_size' 必须为 1
    tensor_model_parallel_size: 1
    data_parallel_size: 1
    expert_parallel_size: 1
    pipeline_model_parallel_size: 1
    max_num_batched_tokens: 8192
    max_num_seqs: 256
    enable_chunked_prefill: true
    enable_prefix_caching: true
    load_format: dummy
    log_prob_use_dynamic_bsz: false
    log_prob_max_token_len_per_gpu: 16384
    disable_log_stats: true
    do_sample: true
    over_sample_rate: 0
    multi_stage_wake_up: false
    calculate_log_probs: true
    skip_rollout: false
    skip_dump_dir: "/tmp/rollout_dump"
    skip_tokenizer_init: true
    mode: sync
    multi_turn:
      enable: false
      max_user_turns: null
      max_parallel_calls: 1
      tool_response_truncate_side: middle
      interaction_config_path: null
      use_inference_chat_template: false
      tokenization_sanity_check_mode: strict
      format: hermes
    val_kwargs:
      _target_: verl.workers.config.SamplingConfig
      top_k: -1
      top_p: 1.0
      temperature: 0
      'n': 1
      do_sample: false

  hybrid_engine: true
  
# Critic配置
# 参考: verl1/config_verl/critic/critic.yaml
critic:
  _target_: verl.workers.config.CriticConfig
  enable: false # GRPO (adv_estimator=grpo) 不需要 critic，设置为 false 是正确的
  strategy: fsdp

# 奖励模型配置
# 参考: verl1/config_verl/reward_model/reward_model.yaml
reward_model:
  enable: False
  enable_resource_pool: false
  use_dynamic_bsz: false
  reward_manager: batch
  # 'model_path' 拼写错误，应为 'model.path'，但因为 enable: false，所以无影响
  # model_path: null 
  temperature: 1.0
  top_p: 1.0
  launch_reward_fn_async: false

# 自定义奖励函数配置
# 参考: verl1/config_verl/ppo_trainer.yaml
custom_reward_function:
  path: "/data/zhuldz/lunwen/rl/train/verl1/custom_reward.py"
  name: compute_custom_reward
  # 'config' 子块不是标准配置。

# 参考: verl1/config_verl/ppo_trainer.yaml
global_profiler:
  _target_: verl.utils.profiler.ProfilerConfig
  tool: null
  steps: null
  profile_continuous_steps: false
  save_path: outputs/profile
  global_tool_config:
    nsys:
      _target_: verl.utils.profiler.config.NsightToolConfig
      discrete: false
      controller_nsight_options:
        trace: cuda,nvtx,cublas,ucx
        cuda-memory-usage: 'true'
        cuda-graph-trace: graph
      worker_nsight_options:
        trace: cuda,nvtx,cublas,ucx
        cuda-memory-usage: 'true'
        cuda-graph-trace: graph
        capture-range: cudaProfilerApi
        capture-range-end: null
        #kill: none
    torch_memory:
      trace_alloc_max_entries: 100000
      stack_depth: 32
      context: all
      stacks: all
      kw_args: {}