import os
import json
import torch
import argparse
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm import tqdm
from collections import Counter
from datetime import datetime

# ================= é…ç½®åŒºåŸŸ =================
# æ¨¡å‹è·¯å¾„ (ä¿æŒæ‚¨æ–‡ä»¶ä¸­çš„è·¯å¾„)
MERGED_MODEL_PATH = "/data/zhuldz/lunwen/rl/train/verl1/a_model_grpo_standard/qwen3_4b_code_generation_iter_0/global_step_420/actor/huggingface"

# é»˜è®¤æ•°æ®é›† (ä¿®æ”¹æ­¤å¤„å¯ç›´æ¥åˆ‡æ¢ 'humaneval' æˆ– 'mbpp')
DEFAULT_DATASET = "mbpp" 

# æ•°æ®é›†é…ç½®ä¸­å¿ƒ
DATASET_CONFIGS = {
    "humaneval": {
        "path": "/data/zhuldz/lunwen/generation/humaneval_pro.json",
        "output_dir": "/data/zhuldz/lunwen/eva/evalplus_results/humaneval/best_prompt",
        "format": "json",
        "keys": {
            "problem": "raw_problem",
            "solution": "raw_solution"
        }
    },
    "mbpp": {
        "path": "/data/zhuldz/lunwen/data/mbpp/mbpp.jsonl",
        "output_dir": "/data/zhuldz/lunwen/eva/evalplus_results/mbpp/best_prompt",
        "format": "jsonl",
        "keys": {
            "problem": "text",  # MBPP ä½¿ç”¨ 'text' å­—æ®µ
            "solution": "code"  # MBPP ä½¿ç”¨ 'code' å­—æ®µ
        }
    }
}

# [Oracle æ¨¡å¼] æ¨¡æ¿ (ä¿æŒåŸæ ·ï¼Œä¸è®­ç»ƒå¯¹é½)
ZERO_SHOT_TEMPLATE = """I will provide you with some examples of generating system prompts. Please carefully study and understand the content and structure of these examples.\n\nBased on the examples above, generate an English system prompt for the following input (follow the same format as examples),IMPORTANT RULES:\nOutput ONLY the final system prompt, with NO intermediate thinking, explanations, or reasoning.\nDo NOT include phrases like 'Let me think', 'First, I need to', or any similar thought process.\nIt is not allowed to output any thinking and explanatory statements, only the generated system prompts:

ã€Inputã€‘
Original prompt: {raw_problem}
Correct code: {raw_solution}
"""

# ===========================================

def load_data(dataset_name, limit=50):
    """åŠ è½½å¹¶æ ‡å‡†åŒ–æ•°æ®æ ¼å¼"""
    if dataset_name not in DATASET_CONFIGS:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†: {dataset_name}")
    
    config = DATASET_CONFIGS[dataset_name]
    path = config["path"]
    
    if not os.path.exists(path):
        raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {path}")
    
    print(f"ğŸ“š Loading {dataset_name} data from: {path}")
    
    raw_data = []
    # JSON æ ¼å¼ (åˆ—è¡¨)
    if config["format"] == "json":
        with open(path, 'r', encoding='utf-8') as f:
            raw_data = json.load(f)
    # JSONL æ ¼å¼ (æ¯è¡Œä¸€ä¸ªå¯¹è±¡)
    elif config["format"] == "jsonl":
        with open(path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    raw_data.append(json.loads(line))
    
    # æå–ç»Ÿä¸€æ ¼å¼
    processed_data = []
    keys = config["keys"]
    
    # æˆªå–å‰ N æ¡
    data_to_process = raw_data[:limit] if len(raw_data) > limit else raw_data
    
    for item in data_to_process:
        prob = item.get(keys["problem"])
        sol = item.get(keys["solution"])
        if prob and sol:
            processed_data.append({
                "raw_problem": prob,
                "raw_solution": sol
            })
            
    return processed_data, config["output_dir"]

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset", type=str, default=DEFAULT_DATASET, choices=["humaneval", "mbpp"], help="é€‰æ‹©æ•°æ®é›†")
    parser.add_argument("--limit", type=int, default=50, help="æå–æ ·æœ¬æ•°é‡")
    args = parser.parse_args()

    print(f"ğŸš€ Loading Model from: {MERGED_MODEL_PATH}")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(MERGED_MODEL_PATH, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            MERGED_MODEL_PATH, 
            torch_dtype=torch.bfloat16, 
            device_map="auto",
            trust_remote_code=True
        )
    except OSError as e:
        print(f"âŒ åŠ è½½å¤±è´¥ï¼šåœ¨ {MERGED_MODEL_PATH} æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ã€‚")
        print(f"é”™è¯¯è¯¦æƒ…: {e}")
        return

    # åŠ è½½æ•°æ®
    eval_data, output_dir = load_data(args.dataset, args.limit)
    
    # å‡†å¤‡è¾“å‡ºæ–‡ä»¶
    if not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)
        print(f"ğŸ“ åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
    
    timestamp = datetime.now().strftime("%m%d_%H%M")
    output_file = os.path.join(output_dir, f"extracted_{args.dataset}_{timestamp}.jsonl")

    extracted_prompts = []
    print(f"ğŸ”„ å¼€å§‹ä¸º {len(eval_data)} æ¡æ•°æ®æå– System Prompt...")

    for item in tqdm(eval_data):
        # æ„é€ è¾“å…¥
        input_text = ZERO_SHOT_TEMPLATE.format(
            raw_problem=str(item['raw_problem']).strip(),
            raw_solution=str(item['raw_solution']).strip()
        )
        
        inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=512, # é•¿åº¦ç»™å¤Ÿï¼Œä¿ç•™å®Œæ•´è¾“å‡ºä»¥ä¾¿åˆ†æ
                do_sample=False,    # è´ªå©ªè§£ç 
                temperature=0.0
            )
        
        generated_full = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # æå–ç”Ÿæˆéƒ¨åˆ† (å»æ‰ Input)
        generated_part = generated_full[len(input_text):].strip()
        
        # ã€ä¿ç•™åŸå§‹è¾“å‡ºã€‘ä¸åš split æ¸…æ´—ï¼Œåªä¿å­˜æ¨¡å‹åå‡ºçš„å®Œæ•´å†…å®¹
        extracted_prompts.append(generated_part)

    # ç»Ÿè®¡ä¸åˆ†æ
    print("\n" + "="*50)
    print(f"ğŸ“Š {args.dataset} æç¤ºè¯ç»Ÿè®¡ (Top 5)")
    print("="*50)
    
    counter = Counter(extracted_prompts)
    most_common = counter.most_common(5)
    
    for i, (prompt, count) in enumerate(most_common, 1):
        ratio = count / len(extracted_prompts) * 100
        print(f"\nğŸ† Rank {i} (Count: {count}, Ratio: {ratio:.1f}%)")
        print("-" * 20)
        # åªæ‰“å°å‰200ä¸ªå­—ç¬¦é¢„è§ˆ
        print(prompt[:200] + "..." if len(prompt) > 200 else prompt)
        print("-" * 20)

    # ä¿å­˜ç»“æœ
    with open(output_file, 'w', encoding='utf-8') as f:
        for p in extracted_prompts:
            f.write(json.dumps({"dataset": args.dataset, "generated_system_prompt": p}, ensure_ascii=False) + "\n")
            
    print(f"\nğŸ’¾ æå–ç»“æœå·²ä¿å­˜è‡³: {output_file}")

if __name__ == "__main__":
    main()