import os
import json
import torch
import re
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from tqdm import tqdm
from collections import Counter

# ================= é…ç½®åŒºåŸŸ =================
MERGED_MODEL_PATH = "/data/zhuldz/lunwen/rl/train/verl1/a_model_grpo_standard/qwen3_4b_code_generation_iter_0/global_step_420/actor/huggingface"

# 3. æµ‹è¯•æ•°æ®è·¯å¾„ (æ‚¨çš„æ•°æ®æ–‡ä»¶)
DATA_PATH = "/data/zhuldz/lunwen/generation/humaneval_pro.json"

# 4. è¾“å‡ºæ–‡ä»¶
OUTPUT_LOG = "/data/zhuldz/lunwen/eva/evalplus_results/humaneval/best_prompt/1204_1019.josnl"

# ===========================================

# [Oracle æ¨¡å¼] æ¨¡æ¿ï¼šåŒ…å«é—®é¢˜å’ŒçœŸå€¼ï¼Œæ¨¡æ‹Ÿè®­ç»ƒæ—¶çš„è¾“å…¥åˆ†å¸ƒ
ZERO_SHOT_TEMPLATE = """I will provide you with some examples of generating system prompts. Please carefully study and understand the content and structure of these examples.\n\nBased on the examples above, generate an English system prompt for the following input (follow the same format as examples),IMPORTANT RULES:\nOutput ONLY the final system prompt, with NO intermediate thinking, explanations, or reasoning.\nDo NOT include phrases like 'Let me think', 'First, I need to', or any similar thought process.\nIt is not allowed to output any thinking and explanatory statements, only the generated system prompts:

ã€Inputã€‘
Original prompt: {raw_problem}
Correct code: {raw_solution}
"""

def main():
    print(f"ğŸš€ Loading Full Merged Model from: {MERGED_MODEL_PATH}")
    
    # 1. ç›´æ¥åŠ è½½å…¨é‡æ¨¡å‹
    try:
        tokenizer = AutoTokenizer.from_pretrained(MERGED_MODEL_PATH, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            MERGED_MODEL_PATH, 
            dtype=torch.bfloat16, 
            device_map="auto",
            trust_remote_code=True
        )
    except OSError as e:
        print(f"âŒ åŠ è½½å¤±è´¥ï¼šåœ¨ {MERGED_MODEL_PATH} æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ã€‚")
        print(f"é”™è¯¯è¯¦æƒ…: {e}")
        print("è¯·ç¡®ä¿æ‚¨å·²ç»è¿è¡Œäº†åˆå¹¶è„šæœ¬ï¼Œå¹¶ä¸”è¯¥ç›®å½•ä¸‹æœ‰ config.json æ–‡ä»¶ã€‚")
        return

    # 2. å‡†å¤‡æ•°æ®
    if not os.path.exists(DATA_PATH):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶ {DATA_PATH}")
        return

    with open(DATA_PATH, 'r') as f:
        data = json.load(f)
        # åªå–å‰ 20 æ¡éªŒè¯æ”¶æ•›æ€§
        eval_data = data[:20] if len(data) > 20 else data

    extracted_prompts = []
    print(f"ğŸ”„ å¼€å§‹ä¸º {len(eval_data)} æ¡æ•°æ®æå– System Prompt (Full Model Oracle Mode)...")

    # 3. æ‰¹é‡ç”Ÿæˆ
    for item in tqdm(eval_data):
        # è·å–é—®é¢˜å’ŒçœŸå€¼
        p_text = item.get('raw_problem', '')
        s_text = item.get('raw_solution', '')
        
        if not p_text or not s_text: 
            continue

        # æ„é€ è¾“å…¥
        input_text = ZERO_SHOT_TEMPLATE.format(
            raw_problem=p_text.strip(),
            raw_solution=s_text.strip()
        )
        
        inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=4096,
                do_sample=False, # è´ªå©ªè§£ç 
                temperature=0.05
            )
        
        generated_full = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # æå–ç”Ÿæˆéƒ¨åˆ†
        generated_part = generated_full[len(input_text):].strip()
        
        # æ¸…æ´—æˆªæ–­
        for stop_word in ["Correct code:", "Input:", "Original prompt:", "<|im_end|>"]:
            if stop_word in generated_part:
                generated_part = generated_part.split(stop_word)[0].strip()
            
        extracted_prompts.append(generated_part)

    # 4. ç»Ÿè®¡ä¸åˆ†æ
    print("\n" + "="*20)
    print("ğŸ“Š æç¤ºè¯æ”¶æ•›æƒ…å†µç»Ÿè®¡ (Top 5)")
    print("="*20)
    
    counter = Counter(extracted_prompts)
    most_common = counter.most_common(5)
    
    best_prompt = None
    for i, (prompt, count) in enumerate(most_common, 1):
        ratio = count / len(extracted_prompts) * 100
        print(f"\nğŸ† Rank {i} (å‡ºç° {count} æ¬¡, å æ¯” {ratio:.1f}%):")
        print("-" * 20)
        print(prompt)
        print("-" * 20)
        if i == 1:
            best_prompt = prompt

    # 5. ä¿å­˜ç»“æœ
    with open(OUTPUT_LOG, 'w') as f:
        for p in extracted_prompts:
            f.write(json.dumps({"generated_system_prompt": p}) + "\n")
            
    print(f"\nğŸ’¾ æå–ç»“æœå·²ä¿å­˜è‡³: {OUTPUT_LOG}")
    
    if best_prompt:
        print("\nâœ… æ“ä½œæŒ‡å—ï¼š")
        print("å¦‚æœ Rank 1 çš„æç¤ºè¯çœ‹èµ·æ¥æ˜¯é€šç”¨çš„ï¼ˆä¸åŒ…å«å…·ä½“ä»£ç ç»†èŠ‚ï¼‰ï¼Œ")
        print("è¯·å°†å…¶å¤åˆ¶å¹¶ç²˜è´´åˆ°æ‚¨çš„ eva/ è¯„ä¼°è„šæœ¬ä¸­ä½œä¸º System Promptã€‚")

if __name__ == "__main__":
    main()