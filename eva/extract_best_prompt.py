import os
import json
import torch
import argparse
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm import tqdm
from collections import Counter
from datetime import datetime

# ================= é…ç½®åŒºåŸŸ =================
MERGED_MODEL_PATH = "/data/zhuldz/lunwen/rl/train/verl1/a_model_grpo_7/qwen3_4b_code_generation_iter_0/global_step_1260/actor/huggingface"
DEFAULT_DATASET = "mbpp" 

DATASET_CONFIGS = {
    "humaneval": {
        "path": "/data/zhuldz/lunwen/generation/humaneval_pro.json",
        "output_dir": "/data/zhuldz/lunwen/eva/evalplus_results/humaneval/best_prompt",
        "format": "json",
        "keys": {"problem": "raw_problem", "solution": "raw_solution"}
    },
    "mbpp": {
        "path": "/data/zhuldz/lunwen/data/mbpp/mbpp.jsonl",
        "output_dir": "/data/zhuldz/lunwen/eva/evalplus_results/mbpp/best_prompt",
        "format": "jsonl",
        "keys": {"problem": "text", "solution": "code"}
    }
}

ZERO_SHOT_TEMPLATE = """ generate an English system prompt for the following input,IMPORTANT RULES:\nOutput ONLY the final system prompt, with NO intermediate thinking, explanations, or reasoning.\nDo NOT include phrases like 'Let me think', 'First, I need to', or any similar thought process.\nIt is not allowed to output any thinking and explanatory statements, only the generated system prompts:

ã€Inputã€‘
Original prompt: {raw_problem}
Correct code: {raw_solution}
"""
# ===========================================

def load_data(dataset_name, limit=50):
    if dataset_name not in DATASET_CONFIGS: raise ValueError(f"ä¸æ”¯æŒ: {dataset_name}")
    config = DATASET_CONFIGS[dataset_name]
    path = config["path"]
    if not os.path.exists(path): raise FileNotFoundError(f"ç¼ºå°‘æ–‡ä»¶: {path}")
    
    print(f"ğŸ“š Loading {dataset_name} from: {path}")
    raw_data = []
    if config["format"] == "json":
        with open(path, 'r') as f: raw_data = json.load(f)
    elif config["format"] == "jsonl":
        with open(path, 'r') as f: 
            raw_data = [json.loads(line) for line in f if line.strip()]
            
    processed_data = []
    for item in raw_data[:limit]:
        prob = item.get(config["keys"]["problem"])
        sol = item.get(config["keys"]["solution"])
        if prob and sol: processed_data.append({"raw_problem": prob, "raw_solution": sol})
    return processed_data, config["output_dir"]

def smart_extract(text):
    """
    æ™ºèƒ½æå–é€»è¾‘ï¼šè§£å†³'å¤è¯»æœº'å¯¼è‡´çš„å†…å®¹ä¸¢å¤±é—®é¢˜
    é€»è¾‘é¡ºåºï¼šå…ˆæ‰¾ Output æ ‡è®°ï¼Œä¿ç•™å…¶åå†…å®¹ï¼›ç„¶åå†å¤„ç† Original prompt æˆªæ–­
    """
    if not text: return ""
    
    # 1. [æ‰¾å¤´] ä¼˜å…ˆå®šä½ Output æ ‡è®°
    # å¦‚æœæ¨¡å‹å…ˆå¤è¯»äº† Inputï¼Œè¿™é‡Œä¼šç›´æ¥è·³è¿‡å¤è¯»éƒ¨åˆ†ï¼Œå®šä½åˆ°çœŸæ­£çš„è¾“å‡º
    start_markers = ["ã€Outputã€‘", "Output:", "### Output"]
    for m in start_markers:
        if m in text:
            # å–æ ‡è®°ä¹‹åçš„å†…å®¹ï¼ŒæŠ›å¼ƒå‰é¢çš„å¤è¯»
            text = text.split(m, 1)[-1].strip()
            break
            
    # 2. [å»å°¾] æˆªæ–­æ¨¡å‹å¹»è§‰å‡ºæ¥çš„â€œä¸‹ä¸€é¢˜â€
    # æ­¤æ—¶ text å·²ç»æ˜¯ Output ä¹‹åçš„å†…å®¹äº†ï¼Œå¦‚æœå†å‡ºç° Original promptï¼Œè¯´æ˜æ˜¯ä¸‹ä¸€é¢˜çš„å¼€å§‹
    stop_markers = ["Original prompt:", "ã€Inputã€‘", "<|im_end|>", "Input:"]
    for m in stop_markers:
        if m in text:
            text = text.split(m, 1)[0].strip()
            
    return text

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset", type=str, default=DEFAULT_DATASET, choices=["humaneval", "mbpp"])
    parser.add_argument("--limit", type=int, default=50)
    args = parser.parse_args()

    print(f"ğŸš€ Loading Model: {MERGED_MODEL_PATH}")
    try:
        tokenizer = AutoTokenizer.from_pretrained(MERGED_MODEL_PATH, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(MERGED_MODEL_PATH, torch_dtype=torch.bfloat16, device_map="auto", trust_remote_code=True)
    except Exception as e:
        print(f"âŒ Load Error: {e}"); return

    eval_data, output_dir = load_data(args.dataset, args.limit)
    if not os.path.exists(output_dir): os.makedirs(output_dir, exist_ok=True)
    
    timestamp = datetime.now().strftime("%m%d_%H%M")
    output_file = os.path.join(output_dir, f"extracted_{args.dataset}_{timestamp}.jsonl")

    extracted_prompts = []
    print(f"ğŸ”„ Extracting {len(eval_data)} samples...")

    for item in tqdm(eval_data):
        input_text = ZERO_SHOT_TEMPLATE.format(
            raw_problem=str(item['raw_problem']).strip(),
            raw_solution=str(item['raw_solution']).strip()
        )
        inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
        with torch.no_grad():
            outputs = model.generate(**inputs, max_new_tokens=512, do_sample=False, temperature=0.0)
        
        full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        raw_gen = full_text[len(input_text):].strip()
        
        # --- å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨æ™ºèƒ½æå– ---
        # è¿™å°†ä¿ç•™ Explanation (å¦‚æœå®ƒåœ¨ Output å)ï¼Œä½†å»é™¤ Input å¤è¯»å’Œ Next Sample å¹»è§‰
        final_prompt = smart_extract(raw_gen)
        
        if final_prompt:
            extracted_prompts.append(final_prompt)

    # ç»Ÿè®¡
    print("\n" + "="*50)
    counter = Counter(extracted_prompts)
    for i, (p, c) in enumerate(counter.most_common(3), 1):
        print(f"\nğŸ† Rank {i} (Count: {c}, {c/len(extracted_prompts):.1%})")
        print("-" * 20)
        print(p[:300] + "..." if len(p)>300 else p)

    # ä¿å­˜
    with open(output_file, 'w', encoding='utf-8') as f:
        for p in extracted_prompts:
            f.write(json.dumps({"dataset": args.dataset, "generated_system_prompt": p}, ensure_ascii=False) + "\n")
    print(f"\nğŸ’¾ Saved to: {output_file}")

if __name__ == "__main__":
    main()