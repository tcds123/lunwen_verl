# æ–‡ä»¶å: extract_best_prompt.py
import os
import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from tqdm import tqdm

# ================= é…ç½® =================
# 1. åŸºç¡€æ¨¡å‹è·¯å¾„ (å’Œè®­ç»ƒæ—¶ä¸€è‡´)
BASE_MODEL_PATH = "/data/zhuldz/lunwen/models/Qwen3-4B" 
# 2. è®­ç»ƒåçš„ LoRA/Checkpoint è·¯å¾„ (è¯·ä¿®æ”¹ä¸ºæ‚¨å®é™…çš„ output è·¯å¾„)
ADAPTER_PATH = "/data/zhuldz/lunwen/rl/train/verl1/a_model_grpo_standard/qwen3_4b_code_generation_iter_0/global_step_450/actor/huggingface" 
# 3. æµ‹è¯•æ•°æ® (ä½¿ç”¨ humaneval_pro.json æˆ–è®­ç»ƒæ•°æ®)
DATA_PATH = "/data/zhuldz/lunwen/data/humaneval/humaneval_pro.json"
# 4. è¾“å‡ºæ–‡ä»¶
OUTPUT_LOG = "extracted_prompts.jsonl"
# =======================================

def main():
    print(f"ğŸš€ åŠ è½½åŸºç¡€æ¨¡å‹: {BASE_MODEL_PATH}")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH, 
        torch_dtype=torch.bfloat16, 
        device_map="auto",
        trust_remote_code=True
    )

    if os.path.exists(ADAPTER_PATH):
        print(f"ğŸ”— åŠ è½½ LoRA æƒé‡: {ADAPTER_PATH}")
        model = PeftModel.from_pretrained(model, ADAPTER_PATH)
        model.merge_and_unload() # åˆå¹¶æƒé‡ä»¥åŠ é€Ÿæ¨ç†
    else:
        print("âš ï¸ æœªæ‰¾åˆ° Adapter è·¯å¾„ï¼Œå°†ä½¿ç”¨åŸå§‹æ¨¡å‹è¿›è¡Œæ¨ç†ï¼")

    # åŠ è½½æ•°æ®
    with open(DATA_PATH, 'r') as f:
        data = json.load(f)
        # åªå–å‰ 50 æ¡åšé‡‡æ ·å³å¯ï¼Œçœ‹æ˜¯å¦æ”¶æ•›
        data = data[:50] if len(data) > 50 else data

    results = []
    print("ğŸ”„ å¼€å§‹ç”Ÿæˆ System Prompts...")

    for item in tqdm(data):
        # æ„é€ è¾“å…¥ (å¿…é¡»ä¸è®­ç»ƒæ—¶ CustomReward ä¸­çš„æ ¼å¼ä¸€è‡´)
        # å‡è®¾è®­ç»ƒæ—¶è¾“å…¥åŒ…å«äº† "Original prompt: ..." æ ‡è®°
        prompt_text = item['raw_problem']
        input_text = f"Original prompt: {prompt_text}\nCorrect code:"
        
        inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=200, # System Prompt é€šå¸¸ä¸é•¿
                do_sample=False,    # ä½¿ç”¨è´ªå©ªè§£ç ï¼Œçœ‹æ¨¡å‹æœ€æƒ³è¾“å‡ºä»€ä¹ˆ
                temperature=0.0
            )
        
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # æå–ç”Ÿæˆéƒ¨åˆ† (å»æ‰è¾“å…¥)
        generated_part = generated_text[len(input_text):].strip()
        
        # ç®€å•çš„æ¸…æ´— (å»æ‰å¯èƒ½çš„ artifact)
        clean_prompt = generated_part.split('\n')[0].strip() # å‡è®¾ Prompt æ˜¯ä¸€è¡Œï¼Œæˆ–è€…å–ç¬¬ä¸€æ®µ
        
        results.append(clean_prompt)

    # ä¿å­˜å¹¶åˆ†æ
    with open(OUTPUT_LOG, 'w') as f:
        for p in results:
            f.write(json.dumps({"prompt": p}) + "\n")
    
    print("\nğŸ“Š ç»Ÿè®¡å‡ºç°é¢‘ç‡æœ€é«˜çš„ Prompt:")
    from collections import Counter
    counts = Counter(results)
    for p, c in counts.most_common(5):
        print(f"[{c}æ¬¡] {p}")

    best_prompt = counts.most_common(1)[0][0]
    print(f"\nğŸ† æå–åˆ°çš„æœ€ä½³é€šç”¨ Prompt:\n{best_prompt}")

if __name__ == "__main__":
    main()